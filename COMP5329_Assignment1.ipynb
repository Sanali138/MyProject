{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sanali138/MyProject/blob/master/COMP5329_Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qkw-ENE-esOm"
      },
      "outputs": [],
      "source": [
        "# Import relevant libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L2Yxyht0sYm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "Y8kD85jOhnB1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32667219-bffa-4f8a-c3e7-8019c9f5937a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "train_data = np.load(\"/content/gdrive/MyDrive/COMP5329/Assignment1/train_data.npy\")\n",
        "train_label = np.load(\"/content/gdrive/MyDrive/COMP5329/Assignment1/train_label.npy\")\n",
        "test_data = np.load(\"/content/gdrive/MyDrive/COMP5329/Assignment1/test_data.npy\")\n",
        "test_label = np.load(\"/content/gdrive/MyDrive/COMP5329/Assignment1/test_label.npy\")\n",
        "\n",
        "#Check shape\n",
        "print(train_data.shape)\n",
        "print(train_label.shape)\n",
        "print(test_data.shape)\n",
        "print(test_label.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKabFgPDe9ju",
        "outputId": "782c0a24-4fb1-4c0d-924d-c581a2221494"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 128)\n",
            "(50000, 1)\n",
            "(10000, 128)\n",
            "(10000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = train_data\n",
        "X_mean = np.mean(X)\n",
        "X_std = np.std(X)\n",
        "Scaled_data = (X - X_mean)/X_std\n",
        "\n",
        "print(f\"{train_data.shape} is the shape of the original data.\")\n",
        "print(f\"{Scaled_data.shape} is the shape of the scaled data.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LG3CJ3RQ220E",
        "outputId": "f3d7214e-6143-4bea-94a1-e926b0fffa9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 128) is the shape of the original data.\n",
            "(50000, 128) is the shape of the scaled data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a activation class\n",
        "# for each time, we can initiale a activation function object with one specific function\n",
        "# for example: f = Activation(\"tanh\")  means we create a tanh activation function.\n",
        "# you can define more activation functions by yourself, such as relu!\n",
        "\n",
        "class Activation(object):\n",
        "    def __tanh(self, x):\n",
        "        return np.tanh(x)\n",
        "\n",
        "    def __tanh_deriv(self, a):\n",
        "        # a = np.tanh(x)\n",
        "        return 1.0 - a**2\n",
        "    def __logistic(self, x):\n",
        "        return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "    def __logistic_deriv(self, a):\n",
        "        # a = logistic(x)\n",
        "        return  a * (1 - a )\n",
        "\n",
        "    def __relu(self, x):\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def __relu_deriv(self, a):\n",
        "        return np.where(a <= 0, 0, 1)\n",
        "\n",
        "    def __init__(self,activation='tanh'):\n",
        "        if activation == 'logistic':\n",
        "            self.f = self.__logistic\n",
        "            self.f_deriv = self.__logistic_deriv\n",
        "        elif activation == 'tanh':\n",
        "            self.f = self.__tanh\n",
        "            self.f_deriv = self.__tanh_deriv\n",
        "        elif activation == 'relu':\n",
        "          self.f = self.__relu\n",
        "          self.f_deriv = self.__relu_deriv\n"
      ],
      "metadata": {
        "id": "8g6ASQRHgqFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we define the hidden layer for the mlp\n",
        "# for example, h1 = HiddenLayer(10, 5, activation=\"tanh\") means we create a layer with 10 dimension input and 5 dimension output, and using tanh activation function.\n",
        "# notes: make sure the input size of hiddle layer should be matched with the output size of the previous layer!\n",
        "\n",
        "class HiddenLayer(object):\n",
        "    def __init__(self,n_in, n_out,\n",
        "                 activation_last_layer='tanh',activation='tanh', W=None, b=None):\n",
        "        \"\"\"\n",
        "        Typical hidden layer of a MLP: units are fully-connected and have\n",
        "        sigmoidal activation function. Weight matrix W is of shape (n_in,n_out)\n",
        "        and the bias vector b is of shape (n_out,).\n",
        "\n",
        "        NOTE : The nonlinearity used here is tanh\n",
        "\n",
        "        Hidden unit activation is given by: tanh(dot(input,W) + b)\n",
        "\n",
        "        :type n_in: int\n",
        "        :param n_in: dimensionality of input\n",
        "\n",
        "        :type n_out: int\n",
        "        :param n_out: number of hidden units\n",
        "\n",
        "        :type activation: string\n",
        "        :param activation: Non linearity to be applied in the hidden\n",
        "                           layer\n",
        "        \"\"\"\n",
        "        self.input=None\n",
        "        self.activation=Activation(activation).f\n",
        "\n",
        "        # activation deriv of last layer\n",
        "        self.activation_deriv=None\n",
        "        if activation_last_layer:\n",
        "            self.activation_deriv=Activation(activation_last_layer).f_deriv\n",
        "\n",
        "        # we randomly assign small values for the weights as the initiallization\n",
        "        self.W = np.random.uniform(\n",
        "                low=-np.sqrt(6. / (n_in + n_out)),\n",
        "                high=np.sqrt(6. / (n_in + n_out)),\n",
        "                size=(n_in, n_out)\n",
        "        )\n",
        "        if activation == 'logistic':\n",
        "            self.W *= 4\n",
        "\n",
        "        # we set the size of bias as the size of output dimension\n",
        "        self.b = np.zeros(n_out,)\n",
        "\n",
        "        # we set he size of weight gradation as the size of weight\n",
        "        self.grad_W = np.zeros(self.W.shape)\n",
        "        self.grad_b = np.zeros(self.b.shape)\n",
        "\n",
        "        self.W_v = np.zeros(self.W.shape)\n",
        "        self.b_v = np.zeros(self.b.shape)\n",
        "\n",
        "\n",
        "    # the forward and backward progress (in the hidden layer level) for each training epoch\n",
        "    # please learn the week2 lec contents carefully to understand these codes.\n",
        "    def forward(self, input):\n",
        "        '''\n",
        "        :type input: numpy.array\n",
        "        :param input: a symbolic tensor of shape (n_in,)\n",
        "        '''\n",
        "        lin_output = np.dot(input, self.W) + self.b\n",
        "        self.output = (\n",
        "            lin_output if self.activation is None\n",
        "            else self.activation(lin_output)\n",
        "        )\n",
        "        self.input=input\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, delta, output_layer=False):\n",
        "        self.grad_W = np.atleast_2d(self.input).T.dot(np.atleast_2d(delta))\n",
        "        self.grad_b = delta\n",
        "        if self.activation_deriv:\n",
        "            delta = delta.dot(self.W.T) * self.activation_deriv(self.input)\n",
        "        return delta"
      ],
      "metadata": {
        "id": "RqmAM3ZtkIub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP:\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "\n",
        "    # for initiallization, the code will create all layers automatically based on the provided parameters.\n",
        "    def __init__(self, layers, activation=[None,'relu','relu', 'softmax'], weight_decay=1.0):\n",
        "        \"\"\"\n",
        "        :param layers: A list containing the number of units in each layer.\n",
        "        Should be at least two values\n",
        "        :param activation: The activation function to be used in each layer.\n",
        "        :param weight_decay: The value for weight decay.\n",
        "        \"\"\"\n",
        "        ### initialize layers\n",
        "        self.layers=[]\n",
        "        self.params=[]\n",
        "\n",
        "        self.activation=activation\n",
        "        self.weight_decay=weight_decay\n",
        "        for i in range(len(layers)-1):\n",
        "            last_hidden_layer = False\n",
        "            if i == len(layers)-2:\n",
        "              last_hidden_layer = True\n",
        "\n",
        "            self.layers.append(HiddenLayer(layers[i],layers[i+1],activation[i],activation[i+1],last_hidden_layer=last_hidden_layer))\n",
        "\n",
        "    # forward progress: pass the information through the layers and out the results of final output layer\n",
        "    def forward(self,input):\n",
        "        for layer in self.layers:\n",
        "            output=layer.forward(input)\n",
        "            input=output\n",
        "        return output\n",
        "\n",
        "    # define the objection/loss function, we use mean sqaure error (MSE) as the loss\n",
        "    # you can try other loss, such as cross entropy.\n",
        "    # when you try to change the loss, you should also consider the backward formula for the new loss as well!\n",
        "    def criterion_MSE(self,y,y_hat):\n",
        "        activation_deriv=Activation(self.activation[-1]).f_deriv\n",
        "        # MSE\n",
        "        error = y-y_hat\n",
        "        loss=error**2\n",
        "        # calculate the MSE's delta of the output layer\n",
        "        delta=-2*error*activation_deriv(y_hat)\n",
        "        # return loss and delta\n",
        "        return loss,delta\n",
        "\n",
        "    # backward progress\n",
        "    def backward(self,delta):\n",
        "        delta=self.layers[-1].backward(delta,output_layer=True)\n",
        "        for layer in reversed(self.layers[:-1]):\n",
        "            delta=layer.backward(delta)\n",
        "\n",
        "    # update the network weights after backward.\n",
        "    # make sure you run the backward function before the update function!\n",
        "    def update(self,lr, momentum_of_gamma = 0.5):\n",
        "        for layer in self.layers:\n",
        "            layer.W_v = (momentum_of_gamma * layer.W_v) + (lr * layer.grad_W)\n",
        "            layer.b_v = (momentum_of_gamma * layer.b_v) + (lr * layer.grad_b)\n",
        "            layer.W -= layer.W_v\n",
        "            layer.b -= layer.b_v"
      ],
      "metadata": {
        "id": "ZjjGSVeix_Gu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " def updates(self, lr, SGD_optim):\n",
        "      '''\n",
        "      The method to update the parameters under Stochastic Gradient Descent (SGD).\n",
        "      Updates the weights and bias parameters based on the learning rate and respective gradient. Includes functionality for applying SGD Momentum optimization.\n",
        "\n",
        "      Parameters:\n",
        "      lr (float): The learning rate for the parameter updates.\n",
        "      SGD_optim (dict of str: str): The SGD Optimization values as a dictionary with keys 'Type': as the type of optimisation and 'Parameters': for the optimization parameter value.\n",
        "\n",
        "      Returns:\n",
        "      None\n",
        "      '''\n",
        "\n",
        "      if SGD_optim is None:\n",
        "          for layer in self.layers:\n",
        "            layer.W -= lr * layer.grad_W\n",
        "            layer.b -= lr * layer.grad_b\n",
        "\n",
        "      elif SGD_optim['Type'] == 'Momentum':\n",
        "          for layer in self.layers:\n",
        "              layer.v_W = (SGD_optim['Parameter'] * layer.v_W) + (lr * layer.grad_W)\n",
        "              layer.v_b = (SGD_optim['Parameter'] * layer.v_b) + (lr * layer.grad_b)\n",
        "              layer.W = layer.W - layer.v_W\n",
        "              layer.b = layer.b - layer.v_b\n",
        "\n",
        " def mini_batch(self, X, y, learning_rate=0.1, epochs=100, SGD_optim=None, batch_size=1):\n",
        "        for k in range(epochs):\n",
        "            # Assuming Utils.shuffle is defined correctly elsewhere\n",
        "            X, y = Utils.shuffle(X, y)\n",
        "            num_batches = int(np.ceil(X.shape[0] / batch_size))\n",
        "\n",
        "            for batch_idx in range(num_batches):\n",
        "                start_idx = batch_idx * batch_size\n",
        "                end_idx = min((batch_idx + 1) * batch_size, X.shape[0])\n",
        "                X_batch = X[start_idx:end_idx]\n",
        "                y_batch = y[start_idx:end_idx]\n",
        "\n",
        "                y_hat = self.forward(X_batch)\n",
        "                loss, delta = self.criterion_MSE(y_batch, y_hat)\n",
        "                self.backward(delta)\n",
        "                self.updates(learning_rate, SGD_optim=SGD_optim)\n",
        "\n",
        "            if k % 10 == 0:\n",
        "                print(f'Epoch {k+1}/{epochs}, Loss: {loss}')"
      ],
      "metadata": {
        "id": "oDAc5h9fsb7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  def batch_normalize(self, Z):\n",
        "        mean = Z.mean(axis=0)\n",
        "        variance = Z.var(axis=0)\n",
        "        Z_norm = (Z - mean) / np.sqrt(variance + 1e-8)\n",
        "        self.normalized_output = Z_norm\n",
        "        self.mean = mean\n",
        "        self.variance = variance\n",
        "        return self.gamma * Z_norm + self.beta"
      ],
      "metadata": {
        "id": "YAExWBLg108c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gelu(x):\n",
        "    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * np.power(x, 3))))\n",
        "\n",
        "def gelu_deriv(x):\n",
        "    cdf = 0.5 * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * np.power(x, 3))))\n",
        "    pdf = np.exp(-0.5 * x**2) / np.sqrt(2 * np.pi)\n",
        "    return cdf + x * pdf * (np.sqrt(2 / np.pi) * (1 + 3 * 0.044715 * x**2))\n"
      ],
      "metadata": {
        "id": "U1Fb80-64k9p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}