{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please include any imports (allowed by Ed) you require throughout your notebook in the first cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import matplotlib as mp\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge,Lasso\n",
    "from sklearn.linear_model import RidgeCV,LassoCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and preprocessing\n",
    "The data file, called `concrete.csv`, is loaded into the same directory as the notebook you are working in. \n",
    "\n",
    "You will need to load this file into numpy arrays for the attribute data and the labels. So that we can test your code more effectively, please complete this task inside the given function scaffold, and have your function return these arrays.\n",
    "\n",
    "Begin by looking at the file and its format. Notice any missing values and how they are encoded in the file. In the returned X array, missing values should be encoded as [np.nan](https://numpy.org/doc/stable/user/misc.html).\n",
    "\n",
    "While there are multiple ways to load the file correctly, a suggested function to use is [`pd.read_csv`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html). Look through the documentation to check which arguments you will need to pass to the function to load the file correctly. If you choose to use this approach, you will need to extract the appropriate numpy arrays from the pandas dataframe, and exclude any headers. \n",
    "\n",
    "The `X` array returned by your function should have shape `(number of examples, number of attributes)`, and the `y` array returned by your function should have shape `(number of examples,)`. We will also test your function with some different datasets with the same data types, delimiters, and encoding of missing values. However, these files may have a different filename, number of examples and/or attributes, so you should not hard code these values in your solution. There will not be any missing target values, and the targets will always be in the final column. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST FUNCTION: test_data_loading\n",
    "def load_concrete_data(filename):\n",
    "    \"\"\"Load the dataset located at the filname string as described above.\"\"\"\n",
    "    attribute_array = np.array([])\n",
    "    target_array = np.array([])\n",
    "    data=pd.read_csv(filename,sep=\";\", na_values=\"?\")\n",
    "    attribute_array= data.iloc[:,:-1].values.astype(np.float64) #extracting all attributes except the target data column\n",
    "    target_array= data.iloc[:, -1].values.astype(np.float64) # target data column\n",
    "    return attribute_array, target_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 540.     0.     0.   162.     2.5 1040.     nan   28. ]\n",
      " [ 300.     0.     0.   184.     0.  1075.   795.     7. ]\n",
      " [ 251.8    0.    99.9  146.1   12.4 1006.   899.8   56. ]\n",
      " [ 203.5  305.3    0.   203.5    0.   963.4  630.     7. ]\n",
      " [ 149.   236.     0.   176.    13.   847.   893.    28. ]] [79.99 15.58 44.14 19.54 32.96]\n"
     ]
    }
   ],
   "source": [
    "### SKIP\n",
    "# This cell won't be marked. Use it to try out your code.\n",
    "def load_concrete_data(filename):\n",
    "    \"\"\"Load the dataset located at the filname string as described above.\"\"\"\n",
    "    attribute_array = [],\n",
    "    target_array =[]\n",
    "    data=pd.read_csv(filename,sep=\";\", na_values=\"?\")\n",
    "    attribute_array= data.iloc[:,:-1].values.astype(np.float64) #extracting all attributes except the target data column\n",
    "    target_array= data.iloc[:, -1].values.astype(np.float64) # target data column\n",
    "    return attribute_array, target_array\n",
    "    \n",
    "attribute_array, target_array = load_concrete_data('concrete.csv')\n",
    "print(attribute_array[:5], target_array[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next job is to deal with any missing values in the data. Complete the function below which should replace any missing values with the mean value for that attribute. Take a look at the following documentation to see a suitable function to perform this using sklearn: [Documentation](https://scikit-learn.org/stable/modules/impute.html#impute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST FUNCTION: test_missing_vals\n",
    "def fill_missing_values(X):\n",
    "    \"\"\"Fill missing (np.nan) values in the input array as described above.\"\"\"\n",
    "    missing_field = SimpleImputer (missing_values=np.nan, strategy='mean')\n",
    "    data_new = missing_field.fit_transform(X)\n",
    "    return data_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 540.            0.            0.          162.            2.5\n",
      "  1040.          775.20388514   28.        ]\n",
      " [ 300.            0.            0.          184.            0.\n",
      "  1075.          795.            7.        ]\n",
      " [ 251.8           0.           99.9         146.1          12.4\n",
      "  1006.          899.8          56.        ]\n",
      " [ 203.5         305.3           0.          203.5           0.\n",
      "   963.4         630.            7.        ]\n",
      " [ 149.          236.            0.          176.           13.\n",
      "   847.          893.           28.        ]] [79.99 15.58 44.14 19.54 32.96]\n"
     ]
    }
   ],
   "source": [
    "### SKIP\n",
    "# This cell won't be marked. Use it to try out your code.\n",
    "def fill_missing_values(X):\n",
    "    \"\"\"Fill missing (np.nan) values in the input array as described above.\"\"\"\n",
    "    missing_field = SimpleImputer (missing_values=np.nan, strategy='mean')\n",
    "    data_new = missing_field.fit_transform(X)\n",
    "    return data_new\n",
    "attribute_new_array = fill_missing_values(attribute_array)\n",
    "print(attribute_new_array[:5], target_array[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the algorithms will benefit from scaling/transforming the features to a similar range. In Part 1 of the assignment and the labs thus far, we have used min-max scaling to achieve this. Here we would like to utilise a different approach (see the related question in the quiz component).\n",
    "\n",
    "There are various other options implemented in scikit-learn (you might find [this documentation](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html) useful to understand the difference between them). Try utilising [`sklearn.preprocessing.PowerTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PowerTransformer.html#sklearn.preprocessing.PowerTransformer) with the 'yeo-johnson' transformation to transform the data so that features have a zero mean and variance of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST FUNCTION: test_scaling\n",
    "def scale_data(X):\n",
    "    \"\"\"Scale data using PowerTransformer as described above.\"\"\"\n",
    "    transformer = PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "    attribute_transformed = transformer.fit_transform(X)\n",
    "\n",
    "    return attribute_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SKIP\n",
    "# This cell won't be marked. Use it to try out your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you are ready to load and preprocess the dataset. Load the `concrete.csv` file and preprocess it using the functions above. Name your resulting numpy arrays `X` and `y` for the attribute data and targets respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST FUNCTION: test_preprocessing_usage\n",
    "# We will check the values of variables X and y are correct\n",
    "attribute_array, target_array = load_concrete_data('concrete.csv')\n",
    "attribute_new_array = fill_missing_values(attribute_array)\n",
    "X = scale_data(attribute_new_array)\n",
    "y = target_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST FUNCTION: test_preprocessing_hidden\n",
    "# Do not use or delete this cell.\n",
    "# Your functions above will be tested here with several datasets\n",
    "# different to the ones given. If you are failing test_preprocessing_hidden,\n",
    "# make sure your functions work with other datasets as outlined above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing algorithms and evaluating their performance with cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following components, you are required to implement functions which create algorithms and evaluate them with 10 fold cross validation. \n",
    "\n",
    "In order to make this reproducible, it is important that the folds are kept consistent across runs. You can utilise `sklearn.model_selection.KFold` ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold)) to provide consistency in the generation of the 10 folds. This object can be passed into the `cv` argument of `cross_val_score`. Make sure to shuffle the data in this process, and set `random_state` to be 0. Use the variable name `cvKFold`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST FUNCTION: test_cvkfold\n",
    "\n",
    "cvKFold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "pipeline = make_pipeline(PowerTransformer(method='yeo-johnson', standardize=True),\n",
    "    KNeighborsRegressor()\n",
    ")\n",
    "scores = cross_val_score(pipeline, X, y, cv=cvKFold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted KNN Regression\n",
    "We have seen how to implement a KNN classifier in the lab and first part of the assignment. Now your task is to implement a KNN for regression using `sklearn.neighbors.KNeighborsRegressor` ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html)).\n",
    "\n",
    "Use the function definition below, so that algorithm hyperparameters such as number of neighbors, power of minkowski distance, and weighting strategy can optionally be passed in and accessed as a dictionary. Note you also can pass arguments as a dictionary to functions (such as sklearn constructors) using the ** syntax. \n",
    "\n",
    "You should create an instance of the model using the supplied hyperparameters and run 10 fold cross validation (using the fold set up above and your preprocessed data X and y) to evaluate it. The function should return the model instance and the average cross validation R^2 score obtained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST FUNCTION: test_knn\n",
    "def evaluate_knn_regression(X, y, **hyperparams):\n",
    "    cvKFold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "    knn_model = KNeighborsRegressor(**hyperparams)\n",
    "    r2 = cross_val_score(knn_model, X, y, cv=cvKFold, scoring='r2')\n",
    "    mean_r2_score = np.mean(r2)\n",
    "    return knn_model, mean_r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SKIP\n",
    "# Try out your function here - this code won't be marked\n",
    "def evaluate_knn_regression(X, y, **hyperparams):\n",
    "    cvKFold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "    knn_model = KNeighborsRegressor(**hyperparams)\n",
    "    scale_da = make_pipeline(PowerTransformer(method='yeo-johnson', standardize=True), knn_model)\n",
    "    r2 = cross_val_score(knn_model, X, y, cv=cvKFold, scoring='r2')\n",
    "\n",
    "    mean_r2_score = np.mean(r2)\n",
    "\n",
    "    return knn_model, mean_r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now its time to try out your KNN regression using a weighted averaging strategy, which can often improve the performance. Use 9 nearest neighbours with the Euclidean distance. First print the score without weighting. Then print the score using a  weighting strategy based on the inverse of the distance.\n",
    "The format of your output should be:\n",
    "```\n",
    "Cross validation R^2 score without weighting: x.xx\n",
    "Cross validation R^2 score with weighting: x.xx\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation R^2 score without weighting: 0.77\n",
      "Cross validation R^2 score with weighting: 0.81\n"
     ]
    }
   ],
   "source": [
    "### TEST FUNCTION: test_weighted_knn_usage\n",
    "hyperparams_knn = {'n_neighbors': 9, 'weights': 'uniform', 'p': 2}\n",
    "knn_model_without_weighting, r2_score_without_weighting = evaluate_knn_regression(X, y, **hyperparams_knn)\n",
    "knn_weighted_hyperparams = {'n_neighbors': 9, 'weights': 'distance', 'p': 2}\n",
    "model_with_weighting, r2_score_with_weighting = evaluate_knn_regression(X, y, **knn_weighted_hyperparams)\n",
    "print(f\"Cross validation R^2 score without weighting: {r2_score_without_weighting:.2f}\")\n",
    "print(f\"Cross validation R^2 score with weighting: {r2_score_with_weighting:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression\n",
    "Implement similar functions to above to create Ridge and Lasso regression models and perform cross validation with them. They should return the model you have created and the cross validation R^2 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST FUNCTION: test_ridge_regression\n",
    "def evaluate_ridge(X, y, **hyperparams):\n",
    "    ridge_model = Ridge(**hyperparams)\n",
    "    scores = cross_val_score(ridge_model, X, y, cv=cvKFold, scoring='r2')\n",
    "    mean_r2 = scores.mean()\n",
    "    return ridge_model, mean_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST FUNCTION: test_lasso_regression\n",
    "def evaluate_lasso(X, y, **hyperparams):\n",
    "    lasso_model = Lasso(**hyperparams)\n",
    "    scores = cross_val_score(lasso_model, X, y, cv=cvKFold, scoring='r2')\n",
    "    mean_score = scores.mean()\n",
    "    return lasso_model, mean_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SKIP\n",
    "# Try out your function here - this code won't be marked\n",
    "# You can also use this cell to answer the related question in Part 2 Questions\n",
    "# Assuming you have already loaded your data into X and y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform cross validation for ridge and lasso regression, both with alpha=10. Ensure to pass a `random_state` hyperparameter set to 0 for reproducibility. The format of your output should be:\n",
    "\n",
    "```\n",
    "Ridge cross validation R^2 score: x.xx\n",
    "Lasso cross validation R^2 score: x.xx\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge cross validation R^2 score: 0.76\n",
      "Lasso cross validation R^2 score: -0.02\n"
     ]
    }
   ],
   "source": [
    "### TEST FUNCTION: test_ridge_lasso_usage\n",
    "ridgehyperparams = {'alpha': 10.0, 'random_state': 0}\n",
    "lassohyperparams = {'alpha': 10.0,  'random_state': 0 }\n",
    "ridge_model, ridge_r2_score = evaluate_ridge(X, y, **ridgehyperparams)\n",
    "lasso_model, lasso_r2_score = evaluate_lasso(X, y, **lassohyperparams)\n",
    "\n",
    "print(f\"Ridge cross validation R^2 score: {ridge_r2_score:.2f}\", )\n",
    "print(f\"Lasso cross validation R^2 score: {lasso_r2_score:.2f}\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "Decision trees can also be used for regression problems. However in this case we can't choose the best splits on the attribute values using entropy as this relied on looking at the distribution of classes across the resulting subsets of data. Several different criteria can be used to determine the best attribute split. Read through this [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) to take a look at these and understand the class you will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST FUNCTION: test_decision_tree\n",
    "def evaluate_decision_tree(X, y, **hyperparams):\n",
    "    regressor = DecisionTreeRegressor(**hyperparams)\n",
    "    regressor.fit(X,y)\n",
    "    score = cross_val_score(regressor, X, y, cv= cvKFold, scoring='r2')\n",
    "    mean_r2 = score.mean()\n",
    "    return regressor, mean_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.51079008 0.66957267 0.78828521 0.74504584 0.45295744] 0.6333302468905162\n"
     ]
    }
   ],
   "source": [
    "### SKIP\n",
    "# Try out your function here - this code won't be marked\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "criterion = 'friedman_mse'\n",
    "max_depth = 50\n",
    "random_state =0\n",
    "\n",
    "regressor = DecisionTreeRegressor(criterion=criterion, max_depth=max_depth, random_state=random_state)\n",
    "regressor.fit(X_train, y_train)\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "scores = cross_val_score(regressor, X, y, scoring='r2',error_score='raise')\n",
    "mean_r2 = scores.mean()\n",
    "print(scores, mean_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test out your decision tree with the Friedman mean squared error criterion and a max_depth of 50. Make sure to set the `random_state` to 0 so that we can check your output. Print your output in the format:\n",
    "```\n",
    "Cross validation R^2 score: x.xx\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation R^2 score: 0.70\n"
     ]
    }
   ],
   "source": [
    "### TEST FUNCTION: test_decision_tree_usage\n",
    "decision_tree_hyperparams = {\n",
    "    'criterion': 'friedman_mse',  \n",
    "    'max_depth': 50,\n",
    "    'random_state': 0\n",
    "}\n",
    "\n",
    "decision_tree_regressor, decision_tree_r2_score = evaluate_decision_tree(X, y, **decision_tree_hyperparams)\n",
    "print(f\"Cross validation R^2 score: {decision_tree_r2_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Methods\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Random Forest\n",
    "Building on the single decision tree, we can utilise the Random Forest ensemble method.\n",
    "\n",
    "This time, your function that performs cross validation should take an extra argument to specify the metric to be used for the evaluation. See ([Model evaluation](https://scikit-learn.org/0.15/modules/model_evaluation.html)) and ([cross_val_score](https://scikit-learn.org/0.15/modules/model_evaluation.html)) for detail on how different metrics can be used in sklearn evaluations. During testing we will pass strings \"`neg_mean_absolute_error`\", \"`neg_mean_squared_error`\", and \"`r2`\" to specify the evaluation metric expected to be used. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST FUNCTION: test_random_forest\n",
    "def evaluate_random_forest(X, y, metric=None, **hyperparams):\n",
    "    regressor = RandomForestRegressor(**hyperparams)\n",
    "    scores = cross_val_score(regressor, X, y, cv=cvKFold, scoring=metric)\n",
    "    mean_score = scores.mean()\n",
    "    return regressor, mean_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation negative MAE: 6.71\n",
      "Cross validation negative MSE: 73.12\n",
      "Cross validation R^2 score: 0.72\n"
     ]
    }
   ],
   "source": [
    "### SKIP\n",
    "# Try out your function here - this code won't be marked\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Assuming you have your dataset loaded into X and y\n",
    "\n",
    "# Define the Random Forest Regressor with specified hyperparameters\n",
    "rf_regressor = RandomForestRegressor(\n",
    "    criterion='friedman_mse',\n",
    "    max_features=4,\n",
    "    max_leaf_nodes=10,\n",
    "    random_state=0  # Set a random state for reproducibility\n",
    ")\n",
    "\n",
    "# Define a list of evaluation metrics\n",
    "evaluation_metrics = {\n",
    "    'negative MAE': 'neg_mean_absolute_error',\n",
    "    'negative MSE': 'neg_mean_squared_error',\n",
    "    'R^2 score': 'r2'\n",
    "}\n",
    "\n",
    "# Perform cross-validation for each metric and print the results\n",
    "for metric_name, metric_scoring in evaluation_metrics.items():\n",
    "    scores = cross_val_score(rf_regressor, X, y, cv=3, scoring=metric_scoring)\n",
    "    avg_score = -scores.mean() if 'negative' in metric_name else scores.mean()\n",
    "    print(f'Cross validation {metric_name}: {avg_score:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform cross validation of a random forest ensemble which uses the Friedman mean squared error as the criterion, considers a maximum of 4 features when looking for the best split, and grows trees to have a max of 10 leaf nodes.\n",
    "Perform the evaluation 3 times, once using each of the metrics that can be passed to your evaluation function.\n",
    "Your ouput should have the format:\n",
    "```\n",
    "Cross validation negative MAE: x.xx\n",
    "Cross validation negative MSE: x.xx\n",
    "Cross validation R^2 score: x.xx\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation negative MAE: -6.78\n",
      "Cross validation negative MSE: -74.39\n",
      "Cross validation R^2 score: 0.71\n"
     ]
    }
   ],
   "source": [
    "### TEST FUNCTION: test_random_forest_usage\n",
    "hyperparameters = {\n",
    "    'criterion': 'friedman_mse',\n",
    "    'max_features': 4,\n",
    "    'max_leaf_nodes': 10,\n",
    "    'random_state': 0  \n",
    "}\n",
    "\n",
    "evaluation_metrics = {\n",
    "    'negative MAE': 'neg_mean_absolute_error',\n",
    "    'negative MSE': 'neg_mean_squared_error',\n",
    "    'R^2 score': 'r2'\n",
    "}\n",
    "for metric_name, metric_scoring in evaluation_metrics.items():\n",
    "    _, avg_score = evaluate_random_forest(X, y, metric=metric_scoring, **hyperparameters)\n",
    "    print(f'Cross validation {metric_name}: {avg_score:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are questions related to these metrics in \"Part 2 Questions\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning using Grid Search with Cross Validation\n",
    "Many of the regression algorithms we have studied have several important hyperparameters that need to be tuned to obtain effective performance. One way we have showed you how to perform this is grid search with cross validation. For this part, you will need to perform grid search with cross validation for the gradient boosting regression algorithm.\n",
    "\n",
    "We need to split our data into a training and test set to perform this procedure appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST FUNCTION: test_assert_splitting\n",
    "# These variables can be used when performing your grid search later.\n",
    "\n",
    "# TODO: comment the next line out and uncomment the following line to create the \n",
    "# train test split after you've defined X and y earlier in the notebook\n",
    "#X_train, X_test, y_train, y_test = None, None, None, None\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient boosting\n",
    "Write a method to perform a cross validation grid search ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)) to tune over hyperparameters for a gradient boosting model ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html)). You should continue to use the CVKFold you defined above to set up the folds.\n",
    "\n",
    "Your function should return the fitted sklearn CVGridSearch object itself, the best hyperparameter combination (out of those tuned over) as a dictionary (eg. `{\"learning_rate\": 0.1, \"n_estimators\": 100, ...}`) and the best cross validation score found during training. Ensure the best model returned as part of the CVGridSearch object has been trained on the entire training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST FUNCTION: test_gradient_boosting\n",
    "def best_gradient_boosting(X, y, param_grid, metric=None):\n",
    "    grid_search = GridSearchCV(\n",
    "        GradientBoostingRegressor(random_state=0),\n",
    "        param_grid=param_grid,\n",
    "        cv=cvKFold,\n",
    "        scoring=metric if metric is not None else 'r2',\n",
    "        return_train_score=True\n",
    "    ).fit(X, y)\n",
    "    best_params = grid_search.best_params_\n",
    "    best_cv_score = grid_search.best_score_\n",
    "    best_model = grid_search.best_estimator_    \n",
    "    return grid_search, best_params, best_cv_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, you should define a grid with three values of learning rates and maximum number of leaf nodes, and the possible combinations for the criterion.\n",
    "\n",
    "Using this parameter grid, run your cross validation grid search with R^2 as the metric and print the best hyperparameter combination and the best model's test set R^2 score. \n",
    "\n",
    "Use the variable names provided in the scaffold. We will ensure your results are reproduced and that you set reasonable parameter values to achieve a threshold R^2. \n",
    "\n",
    "Output format:\n",
    "```\n",
    "Best hyperparameter combination: {'param1': value, 'param2': value, ...}\n",
    "Best model's test set R^2 score: x.xx\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameter combination: {'criterion': 'friedman_mse', 'learning_rate': 0.3, 'max_leaf_nodes': 6}\n",
      "Best model's test set R^2 score: 0.90\n"
     ]
    }
   ],
   "source": [
    "### TEST FUNCTION: test_parameter_tuning\n",
    "\n",
    "param_grid = {\n",
    "        'learning_rate': [0.01, 0.3, 0.5],  \n",
    "        'max_leaf_nodes': [6, 8, 10],   \n",
    "        'criterion': ['friedman_mse', 'squared_error']\n",
    "\n",
    "}\n",
    "\n",
    "best_model, best_params, best_cv_score = best_gradient_boosting(X_train, y_train, param_grid, metric=None)\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "test_score = r2_score(y_test, y_pred)\n",
    "print(\"Best hyperparameter combination:\", best_params)\n",
    "print(\"Best model's test set R^2 score: {:.2f}\".format(test_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
